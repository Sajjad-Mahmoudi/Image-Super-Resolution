{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a84e2d0",
   "metadata": {
    "id": "8a84e2d0"
   },
   "source": [
    "# The requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594fe824",
   "metadata": {},
   "source": [
    "The current implementation is mostly inspired by the github repository, \n",
    "[SRCNN-keras](https://github.com/MarkPrecursor/SRCNN-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RBeWHvXLY4SV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24689,
     "status": "ok",
     "timestamp": 1642617808301,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "RBeWHvXLY4SV",
    "outputId": "6443b379-022d-4534-bdc4-bbe8f8bace44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6a16c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5961,
     "status": "ok",
     "timestamp": 1642618127083,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "0b6a16c4",
    "outputId": "c5ca9a30-e8f2-4b69-f1bc-247654108ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
      "[GCC 7.5.0]\n",
      "torch: 1.10.0+cu111\n",
      "OpenCV: 4.1.2\n",
      "NumPy: 1.19.5\n",
      "Matplotlib: 3.2.2\n",
      "Scikit-Image: 0.18.3\n"
     ]
    }
   ],
   "source": [
    "# check package versions\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy\n",
    "import matplotlib\n",
    "import skimage\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('torch: {}'.format(torch.__version__))\n",
    "print('OpenCV: {}'.format(cv2.__version__))\n",
    "print('NumPy: {}'.format(numpy.__version__))\n",
    "print('Matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('Scikit-Image: {}'.format(skimage.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8555b0",
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1642618179310,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "8b8555b0"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from __future__ import print_function, division\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils, models\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "# python magic function, displays pyplot figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ac42f",
   "metadata": {
    "id": "f59ac42f"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5128968c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7010,
     "status": "ok",
     "timestamp": 1642618189271,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "5128968c",
    "outputId": "0e4f0dd8-7c45-4cb9-da15-1f6a7538d8e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are in the \"RGB\" format\n"
     ]
    }
   ],
   "source": [
    "# check the format of the images\n",
    "sample = cv2.imread('/content/drive/MyDrive/DIV2K/DIV2K_train_HR/0001.png')\n",
    "img = Image.fromarray(sample)\n",
    "print('The images are in the \"{}\" format'.format(img.mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff994ebc",
   "metadata": {
    "id": "ff994ebc"
   },
   "source": [
    "## Give the paths of images in your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f085",
   "metadata": {
    "id": "d094f085"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# these paths are not used to run the notebook because they all were once used to create the patches \n",
    "# so, the patches is already produced and stored as \".h5\" files (see section 3.1)\n",
    "train_path_dataX2bic = r'DIV2K/DIV2K_train_LR_bicubic/X2/'\n",
    "train_path_dataX3bic = r'DIV2K/DIV2K_train_LR_bicubic/X3/'\n",
    "train_path_dataX4bic = r'DIV2K/DIV2K_train_LR_bicubic/X4/'\n",
    "train_path_dataX2un = r'DIV2K/DIV2K_train_LR_unknown/X2/'\n",
    "train_path_dataX3un = r'DIV2K/DIV2K_train_LR_unknown/X3/'\n",
    "train_path_dataX4un = r'DIV2K/DIV2K_train_LR_unknown/X4/'\n",
    "train_path_label = r'DIV2K/DIV2K_train_HR/'\n",
    "\n",
    "valid_path_dataX2bic = r'DIV2K/DIV2K_valid_LR_bicubic/X2/'\n",
    "valid_path_dataX3bic = r'DIV2K/DIV2K_valid_LR_bicubic/X3/'\n",
    "valid_path_dataX4bic = r'DIV2K/DIV2K_valid_LR_bicubic/X4/'\n",
    "valid_path_dataX2un = r'DIV2K/DIV2K_valid_LR_unknown/X2/'\n",
    "valid_path_dataX3un = r'DIV2K/DIV2K_valid_LR_unknown/X3/'\n",
    "valid_path_dataX4un = r'DIV2K/DIV2K_valid_LR_unknown/X4/'\n",
    "valid_path_label = r'DIV2K/DIV2K_valid_HR/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dbc6e",
   "metadata": {
    "id": "a10dbc6e"
   },
   "source": [
    "## Define the functions to be used in preprocessing the images\n",
    "\n",
    "### the function \"prepare_data\" is used to create some random patches of an image that is used for fine tuning the parameters. For each image, the function produces 30 random patches with the size of 32 x 32 and 20 x 20 for the data and its corresponding labels. we crop the label because in the network, the input image is reduced in shape (see section 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ebdfabb",
   "metadata": {
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1642618255444,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "1ebdfabb"
   },
   "outputs": [],
   "source": [
    "# optional parameters\n",
    "Random_Crop = 30\n",
    "Patch_size = 32\n",
    "label_size = 20\n",
    "conv_side = 6   # = (f1 + f2 + f3 - 3) // 2 = 6\n",
    "scale = 2\n",
    "\n",
    "\n",
    "def prepare_data(path_data, path_label):\n",
    "    \n",
    "    name_data = os.listdir(path_data)\n",
    "    name_label = os.listdir(path_label)\n",
    "    name_data = sorted(name_data)\n",
    "    name_label = sorted(name_label)\n",
    "    nums = name_data.__len__()\n",
    "    \n",
    "    # arrays used to save the patches of training image used for fine tuning the parameters\n",
    "    data = np.zeros((nums * Random_Crop, 1, Patch_size, Patch_size), dtype=np.double)\n",
    "    label = np.zeros((nums * Random_Crop, 1, label_size, label_size), dtype=np.double)\n",
    "\n",
    "    for i in range(nums):\n",
    "        # read the high-res image and the corresponding low-res one\n",
    "        name_hr = path_label + name_label[i]\n",
    "        name_lr = path_data + name_data[i]\n",
    "        hr_img = cv2.imread(name_hr, cv2.IMREAD_COLOR)\n",
    "        lr_img = cv2.imread(name_lr, cv2.IMREAD_COLOR)\n",
    "        shape_hr = hr_img.shape\n",
    "        shape_lr = lr_img.shape\n",
    "        \n",
    "        # convert BGR to YCrCb and take the Y channel\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2YCrCb)\n",
    "        hr_img = hr_img[:, :, 0]\n",
    "        lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2YCrCb)\n",
    "        lr_img = lr_img[:, :, 0]\n",
    "\n",
    "        # resize the low-res image to the corresponding high-res one\n",
    "        lr_img = cv2.resize(lr_img, (shape_hr[1], shape_hr[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # produce Random_Crop random coordinate to crop training image\n",
    "        Points_x = np.random.randint(0, min(shape_hr[0], shape_hr[1]) - Patch_size, Random_Crop)\n",
    "        Points_y = np.random.randint(0, min(shape_hr[0], shape_hr[1]) - Patch_size, Random_Crop)\n",
    "\n",
    "        # produce, normalize ,and store the patches\n",
    "        for j in range(Random_Crop):\n",
    "            lr_patch = lr_img[Points_x[j]: Points_x[j] + Patch_size, Points_y[j]: Points_y[j] + Patch_size]\n",
    "            hr_patch = hr_img[Points_x[j]: Points_x[j] + Patch_size, Points_y[j]: Points_y[j] + Patch_size]\n",
    "\n",
    "            lr_patch = lr_patch.astype(float) / 255.\n",
    "            hr_patch = hr_patch.astype(float) / 255.\n",
    "\n",
    "            data[i * Random_Crop + j, 0, :, :] = lr_patch\n",
    "            label[i * Random_Crop + j, 0, :, :] = hr_patch[conv_side: -conv_side, conv_side: -conv_side]\n",
    "\n",
    "    return data, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a78e93",
   "metadata": {
    "id": "59a78e93"
   },
   "source": [
    "### the function \"prepare_crop_data\" is used to create patches of every image that is used to train the network from scratch. The number of patches created for each image depends on the size of the image and the hyper parameters, block_step and block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7576dd07",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1642618259466,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "7576dd07"
   },
   "outputs": [],
   "source": [
    "BLOCK_STEP = 16\n",
    "BLOCK_SIZE = 32\n",
    "\n",
    "\n",
    "def prepare_crop_data(path_data, path_label):\n",
    "    \n",
    "    name_data = os.listdir(path_data)\n",
    "    name_label = os.listdir(path_label)\n",
    "    name_data = sorted(name_data)\n",
    "    name_label = sorted(name_label)\n",
    "    nums = name_data.__len__()\n",
    "    \n",
    "    # lists to save the patches for training the network\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for i in range(nums):\n",
    "\n",
    "        # read the high-res image and the corresponding low-res one\n",
    "        name_hr = path_label + name_label[i]\n",
    "        name_lr = path_data + name_data[i]\n",
    "        hr_img = cv2.imread(name_hr, cv2.IMREAD_COLOR)\n",
    "        lr_img = cv2.imread(name_lr, cv2.IMREAD_COLOR)\n",
    "        shape_hr = hr_img.shape\n",
    "        shape_lr = lr_img.shape\n",
    "\n",
    "        # convert BGR to YCrCb and take the Y channel\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2YCrCb)\n",
    "        hr_img = hr_img[:, :, 0]\n",
    "        lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2YCrCb)\n",
    "        lr_img = lr_img[:, :, 0]\n",
    "        \n",
    "        # resize the low-res image to the corresponding high-res one\n",
    "        lr_img = cv2.resize(lr_img, (shape_hr[1], shape_hr[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        height_num = (shape_hr[0] - (BLOCK_SIZE - BLOCK_STEP) * 2) // BLOCK_STEP\n",
    "        width_num  = (shape_hr[1] - (BLOCK_SIZE - BLOCK_STEP) * 2) // BLOCK_STEP\n",
    "        \n",
    "        # produce, normalize ,and store the patches\n",
    "        for k in range(height_num):\n",
    "            for j in range(width_num):   \n",
    "                h = k * BLOCK_STEP\n",
    "                w = j * BLOCK_STEP\n",
    "\n",
    "                hr_patch = hr_img[h: h + BLOCK_SIZE, w: w + BLOCK_SIZE]\n",
    "                lr_patch = lr_img[h: h + BLOCK_SIZE, w: w + BLOCK_SIZE]\n",
    "                \n",
    "                lr_patch = lr_patch.astype(float) / 255.\n",
    "                hr_patch = hr_patch.astype(float) / 255.\n",
    "\n",
    "                lr = np.zeros((1, Patch_size, Patch_size), dtype=np.double)\n",
    "                hr = np.zeros((1, label_size, label_size), dtype=np.double)\n",
    "\n",
    "                lr[0, :, :] = lr_patch\n",
    "                hr[0, :, :] = hr_patch[conv_side: -conv_side, conv_side: -conv_side]\n",
    "\n",
    "                data.append(lr)\n",
    "                label.append(hr)\n",
    "\n",
    "    data = np.array(data, dtype=float)\n",
    "    label = np.array(label, dtype=float)\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99920e79",
   "metadata": {
    "id": "99920e79"
   },
   "source": [
    "## store the data/patches and read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc565b69",
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1642618264512,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "fc565b69"
   },
   "outputs": [],
   "source": [
    "# This function is used to save image data patches and their label patches to hdf5 file\n",
    "# \"output_filename.h5\" contains data and label\n",
    "def write_hdf5(data, labels, output_filename):\n",
    "\n",
    "    x = data.astype(np.float32)\n",
    "    y = labels.astype(np.float32)\n",
    "\n",
    "    with h5py.File(output_filename, 'w') as h:\n",
    "        h.create_dataset('data', data=x, shape=x.shape)\n",
    "        h.create_dataset('label', data=y, shape=y.shape)\n",
    "        # h.create_dataset()\n",
    "\n",
    "# read the h5 data \n",
    "def read_training_data(file):\n",
    "    with h5py.File(file, 'r') as hf:\n",
    "        data = np.array(hf.get('data'))\n",
    "        label = np.array(hf.get('label'))\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23744c",
   "metadata": {
    "id": "ec23744c"
   },
   "source": [
    "# Build Dataset\n",
    "\n",
    "To train the model from scratch, the function \"prepare_crop_data\" from the section 2.2.2 should be used which returns two arrays, data including arrays with the shape 32 x 32, and label including arrays with the shape 20 x 20\n",
    "\n",
    "To tune the model, the function \"prepare_data\" from the section 2.2.1 should be used which returns two arrays, data including arrays with the shape 32 x 32, and label including arrays with the shape 20 x 20. This function produces 30 random patches for each image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7fb64",
   "metadata": {
    "id": "bcc7fb64"
   },
   "source": [
    "## Create Patches \n",
    "Using \"prepare_data\" function, we produce 30 random patches for each image of train and validation data (800 images for train and 100 images for validation). Finally, for the training, we have an array with the shape (24000, 1, 32, 32) for data and an array with the shape (24000, 1, 20, 20) for labels, and for the validation an array with the shape (3000, 1, 32, 32) for data and an array with the shape (3000, 1, 20, 20) for labels\n",
    "\n",
    "Uncomment the line for which you want to create patches and then put the corresponding names in the cell related to save patches using \"write_hdf5\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab8f4a",
   "metadata": {
    "id": "b5ab8f4a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this is not used to run the notebook because they all were once executed\n",
    "# data patches and all corresponding label patches are stored as \".h5\" file in \n",
    "# a folder called h5\n",
    "\n",
    "# create patches for data and labels for fine tuning (TRAIN_BICUBIC_X2, X3, and X4)\n",
    "data_tune_X2bic, label_tune = prepare_data(train_path_dataX2bic, train_path_label)\n",
    "# data_tune_X3bic, label_tune = prepare_data(train_path_dataX3bic, train_path_label)\n",
    "# data_tune_X4bic, label_tune = prepare_data(train_path_dataX4bic, train_path_label)\n",
    "\n",
    "# create patches for data and labels for fine tuning (TRAIN_UNKNOWN_X2, X3, and X4)\n",
    "# data_tune_X2un, label_tune = prepare_data(train_path_dataX2un, train_path_label)\n",
    "# data_tune_X3un, label_tune = prepare_data(train_path_dataX3un, train_path_label)\n",
    "# data_tune_X4un, label_tune = prepare_data(train_path_dataX4un, train_path_label)\n",
    "\n",
    "# create patches for data and labels for validation (VALID_BICUBIC_X2, X3, X4)\n",
    "data_val_X2bic, label_val = prepare_data(valid_path_dataX2bic, valid_path_label) \n",
    "# data_val_X3bic, label_val = prepare_data(valid_path_dataX3bic, valid_path_label) \n",
    "# data_val_X4bic, label_val = prepare_data(valid_path_dataX4bic, valid_path_label)\n",
    "\n",
    "# create patches for data and labels for validation (VALID_UNKNOWN_X2, X3, X4)\n",
    "# data_val_X2un, label_val = prepare_data(valid_path_dataX2un, valid_path_label) \n",
    "# data_val_X3un, label_val = prepare_data(valid_path_dataX3un, valid_path_label) \n",
    "# data_val_X4un, label_val = prepare_data(valid_path_dataX4un, valid_path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85809c",
   "metadata": {
    "id": "3e85809c"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook\n",
    "# save the data and the corresponding labels as a \".h5\" file using the function \"write_hdf5\" \n",
    "# Note that you should make a folder named \"h5\" in the current working directory\n",
    "write_hdf5(data_tune_X2bic, label_tune, r'h5/X2_BICUBIC_TRAIN.h5')\n",
    "write_hdf5(data_val_X2bic, label_val, r'h5/X2_BICUBIC_VALID.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7bb54",
   "metadata": {
    "id": "eab7bb54"
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968d30a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1642618283606,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "968d30a3"
   },
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class Dataset_SRCNN(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        if self.transform: \n",
    "            x = self.transform(x)    \n",
    "            y = self.transform(y)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79b9f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "4f79b9f7",
    "outputId": "8f83e0a1-85ab-4e4e-e816-c2e874654fda"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'dataset_eval = Dataset_SRCNN(data_eval_X2bic, label_eval)\\nloader_eval = DataLoader(\\n    dataset_eval,\\n    batch_size=256,\\n    num_workers=2,\\n    shuffle=True,\\n    pin_memory=torch.cuda.is_available()\\n)'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# create dataset and dataloader objects for training\n",
    "# read the \"h5\" files for each data you saved in the section 3.1 using read_training_data function\n",
    "# then create Dataset object and the corresponding dataloader\n",
    "d_train, l_train = read_training_data('/content/drive/MyDrive/DIV2K/h5/X2_BICUBIC_TRAIN.h5')\n",
    "d_train, l_train = torch.tensor(d_train), torch.tensor(l_train)\n",
    "dataset_tune = Dataset_SRCNN(d_train, l_train)\n",
    "loader_tune = DataLoader(\n",
    "    dataset_tune,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# create dataset and dataloader objects for validation\n",
    "d_val, l_val = read_training_data('/content/drive/MyDrive/DIV2K/h5/X2_BICUBIC_VALID.h5')\n",
    "d_val, l_val = torch.tensor(d_val), torch.tensor(l_val)\n",
    "dataset_val = Dataset_SRCNN(d_val, l_val)\n",
    "loader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=256,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0dc92",
   "metadata": {
    "id": "9ad0dc92"
   },
   "source": [
    "# Build the network (SRCNN)\n",
    "\n",
    "It is based on the network presented in the paper [Learning a Deep Convolutional Network for Image Super-Resolution](https://link.springer.com/chapter/10.1007/978-3-319-10593-2_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2869a5da",
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1642618292418,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "2869a5da"
   },
   "outputs": [],
   "source": [
    "# create a network class \n",
    "class SRCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, f1=9, f2=3, f3=5, n1=128, n2=64, c=1, mode=None, input_size=None):\n",
    "        super().__init__()\n",
    "        assert mode in ['train', 'tune', 'test'], 'mode parameter must be \"train\", \"tune\", or \"test\"'\n",
    "        if mode in ['train', 'tune'] and input_size != 32:\n",
    "            raise Exception(\"the input size should be 32 while in train/tune mode\")\n",
    "        self.feature = nn.Conv2d(c, n1, f1)  # (1, 32, 32) ==> (128, 24, 24)        \n",
    "        self.remap = nn.Conv2d(n1, n2, f2, padding='same')   # (128, 24, 24) ==> (64, 24, 24)\n",
    "        self.reconstruct = nn.Conv2d(n2, c, f3)  # (64, 24, 24) ==> (1, 20, 20)\n",
    "\n",
    "    # Input: N, C, H, W\n",
    "    # Output: N, C, H-12, W-12\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = torch.nn.functional.relu(x, inplace=True)\n",
    "        x = self.remap(x)\n",
    "        x = torch.nn.functional.relu(x, inplace=True)\n",
    "        x = self.reconstruct(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77192c4",
   "metadata": {
    "id": "d77192c4"
   },
   "source": [
    "# Train/Tune and evaluate the model\n",
    "In this section, we tune the pretrained model parameters called \"SRCNN_keras_weight.h5\"\n",
    "\n",
    "For this project, it was not practical to train the model from scratch because of two reasons: 1) we have 900 images (800 train images and 100 validation images) for 2 degradation operators and 3 downscale factors, so there are large number of high-resolution images which leads to have a huge amount of patches, 2) it takes a few days to train the network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0c59f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1642618302552,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "7f0c59f8",
    "outputId": "612d179d-6684-43b3-ceed-0f240b0c2e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd43b4",
   "metadata": {
    "id": "62bd43b4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# set the required arguments for \"train_tune\" function\n",
    "dataloader_train = loader_tune\n",
    "dataloader_valid = loader_val\n",
    "model = SRCNN(f1=9, f2=3, f3=5, n1=128, n2=64, c=1, mode='tune', input_size=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "nb_epochs_tune = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71180f54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71180f54",
    "outputId": "e00ebf5f-609d-4e71-9293-ced28f0d940d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 24, 24]          10,496\n",
      "            Conv2d-2           [-1, 64, 24, 24]          73,792\n",
      "            Conv2d-3            [-1, 1, 20, 20]           1,601\n",
      "================================================================\n",
      "Total params: 85,889\n",
      "Trainable params: 85,889\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.85\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 1.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# print model\n",
    "summary(model.to(device),(1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd84d7",
   "metadata": {
    "id": "f7cd84d7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# read the pretrained tf weights and layer names\n",
    "def read_tf_weights(path):\n",
    "    weights = {}\n",
    "    keys = []\n",
    "    with h5py.File(path, 'r') as f: \n",
    "        f.visit(keys.append) \n",
    "        for key in keys:\n",
    "            if ':' in key:\n",
    "                print(f[key].name)\n",
    "                weights[f[key].name] = f[key][()]\n",
    "    return weights\n",
    "\n",
    "tf_original_weights = read_tf_weights('/content/drive/MyDrive/DIV2K/SRCNN_keras_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27exF1WbPcWn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27exF1WbPcWn",
    "outputId": "05ed3bfa-45ad-4245-88cc-d68e057d3eeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# load improved model parameters\n",
    "torch_weights = torch.load('/content/drive/MyDrive/DIV2K/improved models/params_after_training_X2_BICUBIC.h5')\n",
    "model.load_state_dict(torch_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59147c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a59147c",
    "outputId": "f4780277-ab5c-4fc8-85f9-340107992dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the torch layers: ['feature', 'remap', 'reconstruct']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DO NOT RUN THIS. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# determine the torch layer names\n",
    "name_layers = model.named_modules()\n",
    "lst_nameLayers = list(name_layers)\n",
    "print('Name of the torch layers:', [lst_nameLayers[i][0] for i in range(1,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf088799",
   "metadata": {
    "id": "cf088799"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# the shape of weights in keras and torch are transposed, so we should transpose tf weights to load in pytorch model\n",
    "model.feature.weight.data = torch.from_numpy(np.transpose(tf_original_weights['/convolution2d_1/convolution2d_1_W:0']))\n",
    "model.feature.bias.data = torch.from_numpy(np.transpose(tf_original_weights['/convolution2d_1/convolution2d_1_b:0']))\n",
    "model.remap.weight.data = torch.from_numpy(np.transpose(tf_original_weights['/convolution2d_2/convolution2d_2_W:0']))\n",
    "model.remap.bias.data = torch.from_numpy(np.transpose(tf_original_weights['/convolution2d_2/convolution2d_2_b:0']))\n",
    "model.reconstruct.weight.data = torch.from_numpy(np.transpose(tf_original_weights['/convolution2d_3/convolution2d_3_W:0']))\n",
    "model.reconstruct.bias.data = torch.from_numpy(np.transpose(tf_original_weights['/convolution2d_3/convolution2d_3_b:0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KWEHW9rosOkc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWEHW9rosOkc",
    "outputId": "13967b49-990f-4008-c73c-c06ca75f1daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  | Avg. Train Loss | Avg. Valid Loss |  Elapsed  \n",
      "----------------------------------------------------------------------\n",
      "   1    |   0.001298   |   0.001085   |   4.41   \n",
      "   2    |   0.001097   |   0.001017   |   4.31   \n",
      "   3    |   0.001061   |   0.001004   |   4.34   \n",
      "   4    |   0.001044   |   0.000980   |   4.35   \n",
      "   5    |   0.001031   |   0.000970   |   4.38   \n",
      "   6    |   0.001021   |   0.000964   |   4.40   \n",
      "   7    |   0.001015   |   0.000957   |   4.43   \n",
      "   8    |   0.001005   |   0.000955   |   4.43   \n",
      "   9    |   0.001003   |   0.000947   |   4.45   \n",
      "  10    |   0.000995   |   0.000941   |   4.46   \n",
      "  11    |   0.000993   |   0.000946   |   4.49   \n",
      "  12    |   0.000988   |   0.000942   |   4.48   \n",
      "  13    |   0.000984   |   0.000955   |   4.47   \n",
      "  14    |   0.000981   |   0.000931   |   4.44   \n",
      "  15    |   0.000981   |   0.000929   |   4.43   \n",
      "  16    |   0.000975   |   0.000928   |   4.40   \n",
      "  17    |   0.000974   |   0.000923   |   4.38   \n",
      "  18    |   0.000972   |   0.000925   |   4.39   \n",
      "  19    |   0.000967   |   0.000934   |   4.39   \n",
      "  20    |   0.000966   |   0.000922   |   4.38   \n",
      "\n",
      "Training/Tuning completes\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# training/tuning phase\n",
    "# select a proper name for the second argument of the command in the line 82 \n",
    "# This name should be related with the current dataset used for training/tuning\n",
    "def train_tune(dataloader_train, dataloader_valid, model, optimizer, criterion, nb_epochs):\n",
    "    \n",
    "    # move the model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # store loss of each epoch for train and validation\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    \n",
    "    # print the header of the result table\n",
    "    print('Start training...\\n')\n",
    "    print(f\"{'Epoch':^7} | {'Avg. Train Loss':^12} | {'Avg. Valid Loss':^12} | {'Elapsed':^9} \")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i_epoch in range(nb_epochs):\n",
    "        \n",
    "        # measure the elapsed time of each epoch\n",
    "        t0_epoch = time.time()\n",
    "        \n",
    "        # reset tracking variables at the beginning of each epoch\n",
    "        total_loss = 0\n",
    "        \n",
    "        # put the model into the training mode\n",
    "        model.train()\n",
    "        \n",
    "        # batch for loop (step = index of each batch)\n",
    "        for step, batch in enumerate(dataloader_train):\n",
    "            \n",
    "            # load batch to device (GPU or CPU)\n",
    "            X_batch, y_batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # perform a forward pass. This will return the reconstructed patch\n",
    "            rec_batch = model(X_batch)\n",
    "            \n",
    "            # compute loss and accumulate the loss values\n",
    "            loss = criterion(rec_batch, y_batch)\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            # perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Calculate the average loss of each epoch for train\n",
    "        avg_train_loss = total_loss / (step + 1)\n",
    "        loss_train.append(avg_train_loss)\n",
    "        \n",
    "        # put the model into the valiadtion mode\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad(): \n",
    "            for step, batch in enumerate(dataloader_valid):\n",
    "                \n",
    "                # load batch to device (GPU or CPU)\n",
    "                X_batch, y_batch = tuple(t.to(device) for t in batch)\n",
    "                \n",
    "                rec_batch = model(X_batch)\n",
    "                \n",
    "                # compute and store loss\n",
    "                loss = criterion(rec_batch, y_batch)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Calculate the average loss of each epoch for validation\n",
    "            avg_valid_loss = total_loss / (step + 1)\n",
    "            loss_valid.append(avg_valid_loss)\n",
    "                \n",
    "        # Calculate time elapsed for the current epoch\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        \n",
    "        # Print the current results\n",
    "        print(f\"{i_epoch + 1:^7} | {avg_train_loss:^12.6f} | {avg_valid_loss:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "                 \n",
    "    # save the parameters of the current trained/tuned model\n",
    "    torch.save(model.state_dict(), 'params_after_training_X2_BICUBIC.h5')\n",
    "    \n",
    "    return loss_train, loss_valid\n",
    "\n",
    "avgLosses_train, avgLosses_valid = train_tune(dataloader_train, dataloader_valid, model, optimizer, criterion, nb_epochs_tune)\n",
    "print('\\nTraining/Tuning completes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e1e4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "523e1e4f",
    "outputId": "2faf97e6-8372-48b2-cc6a-620040b8d095"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXhV1fXw8e/KPEGAJIwBIUzKPAQQqYhTQWpBrVbQqohztQ44VOuvSm19W6tVtEqrVXCoCtZaxCrFAREVFAIiMgqEKcgQEshA5mS9f+wTuISQAXJzQ7I+z3Ofe+8+5+yz7g1kZe+zz96iqhhjjDH+FBToAIwxxjR+lmyMMcb4nSUbY4wxfmfJxhhjjN9ZsjHGGON3lmyMMcb4nSUbc9IRkXkick1d7xtIIrJVRM7zQ70qIt28138Xkd/WZN/jOM+VIvLh8cZZRb2jRCStrus19S8k0AGYpkFEcn3eRgGFQKn3/iZVfb2mdanqBf7Yt7FT1Zvroh4R6QxsAUJVtcSr+3Wgxj9D0/RYsjH1QlVjyl+LyFbgelX9uOJ+IhJS/gvMGNN4WDeaCajybhIR+bWI7AZmikhLEfmviKSLyH7vdaLPMQtF5Hrv9SQR+UJEnvD23SIiFxznvl1EZJGI5IjIxyLynIj88xhx1yTG34vIl159H4pIvM/2q0Rkm4hkiMiDVXw/w0Rkt4gE+5RdLCKrvNdDRWSJiBwQkV0i8qyIhB2jrpdF5A8+7+/1jvlBRCZX2PcnIvKNiGSLyA4RmeqzeZH3fEBEckVkePl363P8GSKyTESyvOczavrdVEVETvOOPyAia0RknM+2sSKy1qtzp4jc45XHez+fAyKSKSKfi4j97qtn9oWbhqAt0Ao4BbgR9+9ypve+E5APPFvF8cOADUA88GfgJRGR49j3DWApEAdMBa6q4pw1ifEK4FqgNRAGlP/y6wX8zau/vXe+RCqhql8DB4FzKtT7hve6FLjL+zzDgXOBX1YRN14MY7x4zge6AxWvFx0ErgZaAD8BbhGRi7xtI73nFqoao6pLKtTdCngfeMb7bE8C74tIXIXPcNR3U03MocB7wIfecb8CXheRnt4uL+G6ZJsBfYAFXvndQBqQALQBfgPYPF31zJKNaQjKgIdVtVBV81U1Q1X/rap5qpoDPAqcVcXx21T1H6paCrwCtMP9UqnxviLSCRgCPKSqRar6BTD3WCesYYwzVfV7Vc0H3gIGeOWXAv9V1UWqWgj81vsOjuVNYCKAiDQDxnplqOpyVf1KVUtUdSvwfCVxVObnXnyrVfUgLrn6fr6Fqvqdqpap6irvfDWpF1xy2qiqr3lxvQmsB37qs8+xvpuqnA7EAH/yfkYLgP/ifTdAMdBLRJqr6n5VXeFT3g44RVWLVfVztUkh650lG9MQpKtqQfkbEYkSkee9bqZsXLdNC9+upAp2l79Q1TzvZUwt920PZPqUAew4VsA1jHG3z+s8n5ja+9bt/bLPONa5cK2YS0QkHLgEWKGq27w4enhdRLu9OP4frpVTnSNiALZV+HzDRORTr5swC7i5hvWW172tQtk2oIPP+2N9N9XGrKq+idm33p/hEvE2EflMRIZ75Y8Dm4APRSRVRO6v2ccwdcmSjWkIKv6VeTfQEximqs053G1zrK6xurALaCUiUT5lHavY/0Ri3OVbt3fOuGPtrKprcb9UL+DILjRw3XHrge5eHL85nhhwXYG+3sC17Dqqaizwd596q2sV/IDrXvTVCdhZg7iqq7djhesth+pV1WWqOh7XxTYH12JCVXNU9W5VTQLGAVNE5NwTjMXUkiUb0xA1w10DOeD1/z/s7xN6LYUUYKqIhHl/Ff+0ikNOJMa3gQtF5EfexfxHqP7/4hvAHbik9q8KcWQDuSJyKnBLDWN4C5gkIr28ZFcx/ma4ll6BiAzFJbly6bhuv6Rj1P0B0ENErhCREBG5HOiF6/I6EV/jWkH3iUioiIzC/YxmeT+zK0UkVlWLcd9JGYCIXCgi3bxrc1m461xVdVsaP7BkYxqiaUAksA/4CvhfPZ33StxF9gzgD8Bs3P1AlTnuGFV1DXArLoHsAvbjLmBXpfyayQJV3edTfg8uEeQA//BirkkM87zPsADXxbSgwi6/BB4RkRzgIbxWgndsHu4a1ZfeCK/TK9SdAVyIa/1lAPcBF1aIu9ZUtQiXXC7Afe/TgatVdb23y1XAVq878WbczxPcAIiPgVxgCTBdVT89kVhM7YldJzOmciIyG1ivqn5vWRnT2FnLxhiPiAwRka4iEuQNDR6P6/s3xpwgm0HAmMPaAu/gLtanAbeo6jeBDcmYxsG60YwxxviddaMZY4zxO+tGq0R8fLx27tw50GEYY8xJZfny5ftUNaGybZZsKtG5c2dSUlICHYYxxpxURKTizBGHWDeaMcYYv7NkY4wxxu8s2RhjjPE7u2ZjjGkQiouLSUtLo6CgoPqdTUBFRESQmJhIaGhojY/xa7Lx7sJ+GggGXlTVP1XYHg68CgzGzaF0ubcmByLyAHAdbtK821V1vlc+Azfv0l5V7eNT1+9xd3yXAXuBSar6gzf53tO4qcfzvPLydS6MMQ1EWloazZo1o3Pnzhx77TsTaKpKRkYGaWlpdOnSpcbH+a0bzVvX4zncpHm9gIneCoW+rgP2q2o34CngMe/YXsAEoDcwBpjus07Iy15ZRY+raj9VHYCbXfYhr/wC3ER83XGrQP6tTj6gMaZOFRQUEBcXZ4mmgRMR4uLiat0C9ec1m6HAJlVN9WZrnYVrefgaj1stEdy06+d6LZHxwCxv5cYtuFlphwKo6iIgs+LJVDXb5200h9fcGA+8qs5XuAWu2tXJJzTG1ClLNCeH4/k5+TPZdODIlQDTOHKlviP2UdUS3FoTcTU89igi8qiI7MBNLV7esqlRXSJyo4ikiEhKenp6dacyxhhTC41qNJqqPqiqHYHXgdtqeewLqpqsqskJCZXeAFutPdkF/Pz5JXy8ds9xHW+MCZyMjAwGDBjAgAEDaNu2LR06dDj0vqioqMpjU1JSuP3226s9xxlnnFEnsS5cuJALL7ywTuqqL/4cILCTI5edTeToZWHL90kTkRAgFjdQoCbHVuV13GqBD9dBXTUWGxnK0i2ZjOgaz3m92vjjFMYYP4mLi2PlypUATJ06lZiYGO65555D20tKSggJqfxXZnJyMsnJydWeY/HixXUT7EnIny2bZUB3EeniLX07Abemua+5wDXe60txqxCqVz5BRMJFpAvu4v7Sqk4mIt193o7Hrctefo6rxTkdyFLVXSfywY4lIjSYDi0i2bIv1x/VG2Pq2aRJk7j55psZNmwY9913H0uXLmX48OEMHDiQM844gw0bNgBHtjSmTp3K5MmTGTVqFElJSTzzzDOH6ouJiTm0/6hRo7j00ks59dRTufLKKymfgf+DDz7g1FNPZfDgwdx+++3VtmAyMzO56KKL6NevH6effjqrVq0C4LPPPjvUMhs4cCA5OTns2rWLkSNHMmDAAPr06cPnn39e59/ZsfitZaOqJSJyGzAfN/R5hqquEZFHgBRVnQu8BLwmIptwF/0neMeuEZG3gLVACXCrqpYCiMibwCggXkTSgIdV9SXgTyLSEzf0eRtuWVhwLZyxuEEGecC1/vrMAEkJ0aTuO+jPUxjTJFz+/JKjyi7s146rhncmv6iUSTOP/vvz0sGJXJbckcyDRdzyz+VHbJt90/DjiiMtLY3FixcTHBxMdnY2n3/+OSEhIXz88cf85je/4d///vdRx6xfv55PP/2UnJwcevbsyS233HLUPSnffPMNa9asoX379owYMYIvv/yS5ORkbrrpJhYtWkSXLl2YOHFitfE9/PDDDBw4kDlz5rBgwQKuvvpqVq5cyRNPPMFzzz3HiBEjyM3NJSIighdeeIHRo0fz4IMPUlpaSl5e3nF9J8fDr/fZqOoHuF/2vmUP+bwuAC47xrGP4tY5r1he6bevqj87Rrni1nuvF0nx0fx7xU5U1UbWGNMIXHbZZQQHuzsvsrKyuOaaa9i4cSMiQnFxcaXH/OQnPyE8PJzw8HBat27Nnj17SExMPGKfoUOHHiobMGAAW7duJSYmhqSkpEP3r0ycOJEXXnihyvi++OKLQwnvnHPOISMjg+zsbEaMGMGUKVO48sorueSSS0hMTGTIkCFMnjyZ4uJiLrroIgYMGHBC301t2AwCdWxgp5Zsy8wjv7iUqDD7eo05XlW1RCLDgqvc3io67LhbMhVFR0cfev3b3/6Ws88+m//85z9s3bqVUaNGVXpMeHj4odfBwcGUlJQc1z4n4v777+cnP/kJH3zwASNGjGD+/PmMHDmSRYsW8f777zNp0iSmTJnC1VdfXafnPZZGNRqtIbhoYAdevnaoJRpjGqGsrCw6dHB3Trz88st1Xn/Pnj1JTU1l69atAMyePbvaY84880xef/11wF0Lio+Pp3nz5mzevJm+ffvy61//miFDhrB+/Xq2bdtGmzZtuOGGG7j++utZsaL+JlOxZOMntty2MY3PfffdxwMPPMDAgQPrvCUCEBkZyfTp0xkzZgyDBw+mWbNmxMbGVnnM1KlTWb58Of369eP+++/nlVfcffLTpk2jT58+9OvXj9DQUC644AIWLlxI//79GThwILNnz+aOO+6o889wLGK/FI+WnJysx7t4mqoyetoizj61NQ9ccFodR2ZM47Vu3TpOO83+z+Tm5hITE4Oqcuutt9K9e3fuuuuuQId1lMp+XiKyXFUrHQNuLZs6JiIEibBpjw1/NsbU3j/+8Q8GDBhA7969ycrK4qabbgp0SHXCLiz4QVJCNOt25QQ6DGPMSeiuu+5qkC2ZE2UtGz9Iio9he2YexaVlgQ7FGGMaBEs2ftAlPprSMmV7Zv3dMGWMMQ2ZJRs/6JcYy8ShHQkJsps6jTEG7JqNX3Rv04w/XtIv0GEYY0yDYS0bPyktUw7kVT0tuTGm4Tj77LOZP3/+EWXTpk3jlltuOeYxo0aNovw2ibFjx3LgwIGj9pk6dSpPPPFEleeeM2cOa9euPfT+oYce4uOPP65N+JVqSEsRWLLxk6te+pobX11e/Y7GmAZh4sSJzJo164iyWbNm1WgyTHCzNbdo0eK4zl0x2TzyyCOcd955x1VXQ2XJxk86towi1ZYaMOakcemll/L+++8fWiht69at/PDDD5x55pnccsstJCcn07t3bx5++OFKj+/cuTP79u0D4NFHH6VHjx786Ec/OrQMAbh7aIYMGUL//v352c9+Rl5eHosXL2bu3Lnce++9DBgwgM2bNzNp0iTefvttAD755BMGDhxI3759mTx5MoWFhYfO9/DDDzNo0CD69u3L+vXrjw7KR6CXIrBrNn6SlBDN7JQisvKLiY0Mrf4AY8xhd94J3kJmdWbAAJg27ZibW7VqxdChQ5k3bx7jx49n1qxZ/PznP0dEePTRR2nVqhWlpaWce+65rFq1in79Kr8uu3z5cmbNmsXKlSspKSlh0KBBDB48GIBLLrmEG264AYD/+7//46WXXuJXv/oV48aN48ILL+TSSy89oq6CggImTZrEJ598Qo8ePbj66qv529/+xp133glAfHw8K1asYPr06TzxxBO8+OKLx/x8gV6KwFo2ftIl3s0Um5purRtjTha+XWm+XWhvvfUWgwYNYuDAgaxZs+aILq+KPv/8cy6++GKioqJo3rw548aNO7Rt9erVnHnmmfTt25fXX3+dNWvWVBnPhg0b6NKlCz169ADgmmuuYdGiRYe2X3LJJQAMHjz40OSdx/LFF19w1VVXAZUvRfDMM89w4MABQkJCGDJkCDNnzmTq1Kl89913NGvWrMq6a8JaNn6SlOBW5Nuy7yADO7UMcDTGnGSqaIH40/jx47nrrrtYsWIFeXl5DB48mC1btvDEE0+wbNkyWrZsyaRJkygoKDiu+idNmsScOXPo378/L7/8MgsXLjyheMuXKTiRJQrqaykCa9n4SadWUdx1Xg9Oa9c80KEYY2ooJiaGs88+m8mTJx9q1WRnZxMdHU1sbCx79uxh3rx5VdYxcuRI5syZQ35+Pjk5Obz33nuHtuXk5NCuXTuKi4sPLQsA0KxZM3Jyjp7iqmfPnmzdupVNmzYB8Nprr3HWWWcd12cL9FIE1rLxk7CQIO44r3ugwzDG1NLEiRO5+OKLD3WnlU/Jf+qpp9KxY0dGjBhR5fGDBg3i8ssvp3///rRu3ZohQ4Yc2vb73/+eYcOGkZCQwLBhww4lmAkTJnDDDTfwzDPPHBoYABAREcHMmTO57LLLKCkpYciQIdx8881HnbMmpk6dyuTJk+nXrx9RUVFHLEXw6aefEhQURO/evbnggguYNWsWjz/+OKGhocTExPDqq68e1zl9+XWJAREZAzwNBAMvquqfKmwPB14FBgMZwOWqutXb9gBwHVAK3K6q873yGcCFwF5V7eNT1+PAT4EiYDNwraoeEJHOwDqgfEjIV6pa5U/rRJYY8HUgr4gdmfn0Tax6PQpjjC0xcLJpMEsMiEgw8BxwAdALmCgivSrsdh2wX1W7AU8Bj3nH9gImAL2BMcB0rz6Al72yij4C+qhqP+B74AGfbZtVdYD3OL4/C47Dsws2cenfF1NWZmsGGWOaNn9esxkKbFLVVFUtAmYB4yvsMx54xXv9NnCuiIhXPktVC1V1C7DJqw9VXQRkVjyZqn6oquVXyL4CEuv6A9VWUkIMhSVl/JCVH+hQjDEmoPyZbDoAO3zep3llle7jJYosIK6Gx1ZlMuB7Fa+LiHwjIp+JyJmVHSAiN4pIioikpKen1+JUx1Y+/HnLvoN1Up8xjZ2tHHxyOJ6fU6MbjSYiDwIlQPlQj11AJ1UdCEwB3hCRo4aIqeoLqpqsqskJCQl1EkvXhPJ7bSzZGFOdiIgIMjIyLOE0cKpKRkYGERERtTrOn6PRdgIdfd4nemWV7ZMmIiFALG6gQE2OPYqITMINHjhXvX+xqloIFHqvl4vIZqAHcOIjAKqR0Cyc6LBgu7HTmBpITEwkLS2NuupZMP4TERFBYmLtrlT4M9ksA7qLSBdcopgAXFFhn7nANcAS4FJggaqqiMzFtUCeBNoD3YGlVZ3MG/l2H3CWqub5lCcAmapaKiJJXl2pdfEBqyMiPHX5AE6Ji66P0xlzUgsNDaVLly6BDsP4id+SjaqWiMhtwHzc0OcZqrpGRB4BUlR1LvAS8JqIbMJd9J/gHbtGRN4C1uK6xG5V1VIAEXkTGAXEi0ga8LCqvgQ8C4QDH7kxBoeGOI8EHhGRYqAMuFlVjxpg4C8/7t22vk5ljDENll/vszlZ1dV9NgC7swr4eksGo3u3JSI0uPoDjDHmJBWQ+2yMs3RrJnfMWmkj0owxTZolGz9LsuHPxhhjycbfbKkBY4yxZON30eEhtG0eQaq1bIwxTZglm3qQlBBtN3YaY5o0W2KgHjwyvg9RYTYSzRjTdFmyqQfdWscEOgRjjAko60arB/tyC/n7Z5ttkIAxpsmylk09yCss5U/z1tMyKpSkBGvlGGOaHmvZ1IMOLSMJCw6yQQLGmCbLkk09CA4STomLsuHPxpgmy5JNPXHDn+2ajTGmabJkU0+SEmLYeSCf0jKb+NQY0/TYAIF68stRXbnrvB4EB0mgQzHGmHpnyaaeNIsIDXQIxhgTMNaNVk+KS8uYOncN/1u9O9ChGGNMvbNkU09Cg4OYs3Inizba+urGmKbHr8lGRMaIyAYR2SQi91eyPVxEZnvbvxaRzj7bHvDKN4jIaJ/yGSKyV0RWV6jrcRFZLyKrROQ/ItKiurrqW1K8jUgzxjRNfks2IhIMPAdcAPQCJopIrwq7XQfsV9VuwFPAY96xvYAJQG9gDDDdqw/gZa+soo+APqraD/geeKAGddWrpIQYu7HTGNMk+bNlMxTYpKqpqloEzALGV9hnPPCK9/pt4FwREa98lqoWquoWYJNXH6q6CMiseDJV/VBVS7y3XwGJPueotK76lpQQzd6cQnILS6rf2RhjGhF/JpsOwA6f92leWaX7eIkiC4ir4bFVmQzMq0UciMiNIpIiIinp6f65rpIUH0Ob5uHszS7wS/3GGNNQNbqhzyLyIFACvF6b41T1BeAFgOTkZL/ceTm6dxvG9Gnrj6qNMaZB82fLZifQ0ed9oldW6T4iEgLEAhk1PPYoIjIJuBC4UlXLE8Zx1eUProfQGGOaHn8mm2VAdxHpIiJhuIv0cyvsMxe4xnt9KbDASxJzgQneaLUuQHdgaVUnE5ExwH3AOFXNq3COWtXlTw+9u5o/zlsXqNMbY0xA+K0bTVVLROQ2YD4QDMxQ1TUi8giQoqpzgZeA10RkE+6i/wTv2DUi8hawFtcldquqlgKIyJvAKCBeRNKAh1X1JeBZIBz4yGtBfKWqN1dVVyBsy8hjX26hG6NnjDFNhBzubTLlkpOTNSUlxS91/+69NcxetoM1vxtt3WrGmEZFRJaranJl22wGgXqWlBBDXlEpu21EmjGmCbFkU8+S4qMB2GI3dxpjmhBLNvWsW+sY+ndsgXVeGmOakkZ3n01D16Z5BO/eOiLQYRhjTL2ylo0xxhi/s2QTAI/PX8/Ypz8PdBjGGFNvLNkEQHBQEOt2Z1NYErDbfYwxpl5ZsgmApPhoVN0NnsYY0xRYsgmApAQ3/NnWtjHGNBWWbAKgi3evTeo+W7XTGNM0WLIJgGYRoVw2OJEucdGBDsUYY+qF3WcTII9f1j/QIRhjTL2xlk0A2fLQxpimwpJNgLy2ZCt9Hp7P/oNFgQ7FGGP8zpJNgLRvEQlA6j4bkWaMafws2QRIUkIMAKnpNiLNGNP4WbIJkMSWkYQEibVsjDFNgl+TjYiMEZENIrJJRO6vZHu4iMz2tn8tIp19tj3glW8QkdE+5TNEZK+IrK5Q12UiskZEykQk2ae8s4jki8hK7/F3/3za2gkNDqJTXJSta2OMaRL8NvRZRIKB54DzgTRgmYjMVdW1PrtdB+xX1W4iMgF4DLhcRHoBE4DeQHvgYxHpoaqlwMvAs8CrFU65GrgEeL6ScDar6oC6+3R144Yzk2gWYaPPjTGNnz9/0w0FNqlqKoCIzALGA77JZjww1Xv9NvCsiIhXPktVC4EtIrLJq2+Jqi7ybQGVU9V13nn88mH8YeLQToEOwRhj6oU/u9E6ADt83qd5ZZXuo6olQBYQV8Nja6OLiHwjIp+JyJmV7SAiN4pIioikpKenn8Cpaq6opIyNe3LsfhtjTKPXFAYI7AI6qepAYArwhog0r7iTqr6gqsmqmpyQkFAvga3ccYDzn1pEytbMejmfMcYEij+TzU6go8/7RK+s0n1EJASIBTJqeGyNqGqhqmZ4r5cDm4Eex1NXXbPZn40xTYU/k80yoLuIdBGRMNwF/7kV9pkLXOO9vhRYoKrqlU/wRqt1AboDS48nCBFJ8AYrICJJXl2px1NXXYuLDqN5RAhbbPizMaaR81uy8a7B3AbMB9YBb6nqGhF5RETGebu9BMR5AwCmAPd7x64B3sINJvgfcKs3Eg0ReRNYAvQUkTQRuc4rv1hE0oDhwPsiMt87x0hglYisxA1CuFlVG0S/lYjQJSHGlhowxjR64hoSxldycrKmpKTUy7mmzF7JktQMljxwbr2czxhj/EVElqtqcmXb7CaPAPvF8FP4Sb92gQ7DGGP8ypJNgA3q1DLQIRhjjN81haHPDVpRSRmffZ9uE3IaYxo1SzYBVlqmXDNjKe99uyvQoRhjjN9YsgmwyLBgOrSIZIuNSDPGNGKWbBqApIRoW2rAGNOoWbJpAJLio0lNP4gNQzfGNFY1SjYiEi0iQd7rHiIyTkRC/Rta05GUEENuYQnpuYWBDsUYY/yipkOfFwFnikhL4EPcVDSXA1f6K7Cm5IK+bRl8SktaRoUFOhRjjPGLmnajiarm4RYnm66ql+EWNjN1oHWzCPp0iCU02Ho1jTGNU42TjYgMx7Vk3vfKgv0TUtP0n2/SWLB+T6DDMMYYv6hpsrkTeAD4jzeZZhLwqf/Canqe/yyV17/aHugwjDHGL2p0zUZVPwM+A/AGCuxT1dv9GVhTk5QQzbpdOYEOwxhj/KKmo9HeEJHmIhINrAbWisi9/g2taUmKj2F7Zh5FJWWBDsUYY+pcTbvReqlqNnARMA/oAlzlt6iaoKSEaErLlO2ZeYEOxRhj6lxNk02od1/NRcBcVS0G7A7EOtQl3i0RvdVmEjDGNEI1vc/meWAr8C2wSEROAbL9FVRT1Lt9LCt+ez4to+xeWWNM41Ojlo2qPqOqHVR1rDrbgLOrO05ExojIBhHZJCL3V7I9XERme9u/FpHOPtse8Mo3iMhon/IZIrJXRFZXqOsyEVkjImUiklxhW6V1NSRhIUG0ig5DRAIdijHG1LmaDhCIFZEnRSTFe/wFiK7mmGDgOeACoBcwUUR6VdjtOmC/qnYDngIe847tBUzA3Tg6Bpju1QfwsldW0WrcTaeLKsRRVV0NytvL05j28feBDsMYY+pcTa/ZzABygJ97j2xgZjXHDAU2qWqqqhYBs4DxFfYZD7zivX4bOFfcn/bjgVmqWqiqW4BNXn2o6iIgs+LJVHWdqm6oJI5j1tXQLNuSyT+/2hboMIwxps7VNNl0VdWHvcSRqqq/A5KqOaYDsMPnfZpXVuk+qloCZAFxNTy2puqyLr9KSohmX24RWfnFgQ7FGGPqVE2TTb6I/Kj8jYiMAPL9E1JgiMiN5d2E6enpAYmhfETaZlsi2hjTyNQ02dwMPCciW0VkK/AscFM1x+wEOvq8T/TKKt1HREKAWCCjhsfWVI3qUtUXVDVZVZMTEhKO81Qnpl9iC8JDgvjjB+soLCkNSAzGGOMPNR2N9q2q9gf6Af1UdSBwTjWHLQO6i0gXEQnDXaSfW2GfucA13utLgQXqVhCbC0zwRqt1AboDS2v0iY5Wl3X5VdvYCB6/rD8JzcIps4kEjDGNSK3mtFfVbG8mAYAp1exbAtwGzAfWAW95k3g+IiLjvN1eAuJEZJNX3/3esWuAt4C1wP+AW1W1FEBE3gSWAD1FJE1ErvPKLxaRNGA48L6IzK+uroZoXP/2PHfFICLDgikptYxjjGkc5HiXIhaRHarasfo9T64eyUkAACAASURBVD7JycmakpIS0Bj2ZBcwaeYy7jyvO6N7tw1oLMYYUxMislxVkyvbdiKrddl0NX4UGxlKWEgQd85ayZofsgIdjjHGnJAqk42I5IhIdiWPHKB9PcV48ti3D/7wB8g68eQQERrMP64aTIuoUK5/JYW92QV1EKAxxgRGlclGVZupavNKHs1UtabzqjUd27bBb38LL75YJ9W1bh7Bi9ckk5VfzA2vLaeguMFeajLGmCrZovd1afBgOOssePppKK6bGzN7t49l2uUDKC4pI9tu9jTGnKQs2dS1e+6BHTvg7bfrrMof927Le7/6Ea2bR3C8AzqMMSaQLNnUtbFjoWdP+MtfoA4TQ3CQkF9Uyi3/XMHcb3+os3qNMaY+WLKpa0FBMGUKLF8OixZVv38tBAcJmQeLuOdf3/LN9v11WrcxxviTJRt/uOoqSEhwrZs6FBYSxN+vGkzb5hHc8Opydh5oVNPTGWMaMUs2/hAZCb/8Jbz3HqxfX6dVt4oO46VrkiksLuX6V1I4WFhSp/UbY4w/WLLxl1/+EsLD4amn6rzq7m2a8eyVg9iXW8j2zLw6r98YY+qaJRt/ad0arr4aXn0V/LBkwVk9Elh079mc1q55nddtjDF1zZKNP02ZAgUFMH26X6qPDAumrEx58qPveWvZjuoPMMaYALFk40+nngoXXgjPPQf5/rmYX6bKN9v38+Cc7/gqNcMv5zDGmBNlycbf7r7bdaP9859+qT4kOIhnrxhEp1ZR3PzP5WzLOOiX8xhjzImwZONvZ50FgwbBk0/irxXRYiNDeemaIQBMfnkZu7JsSLQxpmGxZONvIq51s349fPCB307TOT6av/9iMGn78/nB7r8xxjQwx714WmNW54unFRdDUhJ06wafflp39VZiX24h8THhAPz5f+tpFR3GVcNPITwk2K/nNcYYfy2eZmoqNBTuuAMWLoQVK/x6qvJEU1ambNidwx/eX8c5T3zGOyvSKC2zPyyMMYHh12QjImNEZIOIbBKR+yvZHi4is73tX4tIZ59tD3jlG0RktE/5DBHZKyKrK9TVSkQ+EpGN3nNLr3yUiGSJyErv8ZD/PnEVbrgBmjWr8ylsjiUoSHhp0hBev34YLaNDmfLWt/zkmc9ZvdNW/TTG1D+/JRsRCQaeAy4AegETRaRXhd2uA/arajfgKeAx79hewASgNzAGmO7VB/CyV1bR/cAnqtod+MR7X+5zVR3gPR6pi89Xa7GxLuHMnu2WIKgnI7rFM/fWH/HMxIEUl5YRGxkKQFGJfwYrGGNMZfzZshkKbFLVVFUtAmYB4yvsMx54xXv9NnCuiIhXPktVC1V1C7DJqw9VXQRkVnI+37peAS6qyw9TJ+64wz0/80y9njYoSBjXvz0fTzmLjq2iALjxtRRufWMFW/fZUGljjP/5M9l0AHz/hE/zyirdR1VLgCwgrobHVtRGVXd5r3cDbXy2DReRb0Vknoj0ruxgEblRRFJEJCXdD9PLANCpE1x2GbzwAmRn++ccVXB5HErLlH4dYlmwbi/nPfkZD7+7mn25hfUejzGm6WiUAwTUDbErvxq+AjhFVfsDfwXmHOOYF1Q1WVWTExIS/Bfc3Xe7RPPii/47RzWCg4QpP+7JZ/eO4vIhHfnn19s568+fsnjzvoDFZIxp3PyZbHYCHX3eJ3plle4jIiFALJBRw2Mr2iMi7by62gF7AVQ1W1VzvdcfAKEiEn88H6hOJCfDyJHw9NNQEtjlAVo3j+DRi/vy0V0juaBvO/p2iAVg7Q/Z7D9YFNDYjDGNiz+TzTKgu4h0EZEw3AX/uRX2mQtc472+FFjgtUrmAhO80WpdgO7A0mrO51vXNcC7ACLS1rsOhIgMxX3mwE4idvfdsH07vP12QMMol5QQwxOX9adZRCiqyt3/+pbT//gJD7yzig27cwIdnjGmEfDrTZ0iMhaYBgQDM1T1URF5BEhR1bkiEgG8BgzEXfSfoKqp3rEPApOBEuBOVZ3nlb8JjALigT3Aw6r6kojEAW8BnYBtwM9VNVNEbgNu8erJB6ao6uKq4q7zmzorKiuD006D5s1h6VI3y0ADsmF3Di8v3sI7K3ZSWFLGiG5x3H5Od4YlxQU6NGNMA1bVTZ02g0Al/J5sAJ5/Hm6+GT77zHWrNUD7Dxbx5rLtvLp4G/eO7snPBidysLCEMlWaRYQGOjxjTANjyaaW6iXZ5Oe70WlnnAHvvuvfc52g4lJ3T05ocBAvLNrMM59s4rLkRCad0ZlT4qIDHJ0xpqGw6WoaoshIt3T0e+/Bhg2BjqZKocFBhAa7fypndI3nvNNa89qSbYx6YiHXv7KMxZtsFJsxpmqWbALp1lshLAyeeirQkdRYnw6xTJswkC/vP4fbzu7Giu0HeG7hpkPbS0ptZgJjzNGsG60S9dKNVu6GG9zCatu3gz/v7/GTguJSMg4W0aFFJLuy8hn37Jecd1obfty7DWd0jbPZpo1pQqwbrSGbMgUKCuBvfwt0JMclIjSYDi0iASgoLmNol1bMXbmTa2cuY9AjH3Hr6zYljjEGQgIdQJN32mkwdiw89xzcdx9ERAQ6ouPWJT6a564YREFxKUs2Z/Dh2j18sm4PUWGudfPp+r3s2J/Heae1ob2XoIwxTYN1o1WiXrvRwC2ods458I9/wPXX199564GqHpqT7f5/r2LWMjflXd8OsZzfy3W3ndq2eSBDNMbUERv6XEv1nmxUYfBg1522ejUENd7ezU17c/lo7R4+Wrubb3YcoF+HWN697UeAu5m0a0I0IcGN9/Mb05hVlWysG60hEHFT2PziF/C//7lutUaqW+sYurWO4ZZRXdmbU0B6jpttOq+ohHHPfkFkWDDJp7SkX2IL+ndswYDEFsRG2Q2kxpzsrGVTiXpv2QAUF0NSEnTvDgsW1O+5G4DCklI+WbeXT9btZeWO/WxOd4MKHrjgVG46qyuZB4t4Z0UaAzq2oHf7WCLDbJSbMQ2NtWxOBqGhcPvtbpDA738Pt90GLVsGOqp6Ex4SzNi+7Rjbtx0A2QXFrE7LOrTY26q0A/zh/XWAWyKhe+sYBnRswY0jk0hKiAlY3MaYmrGWTSUC0rIByMmByy+HefMgOhomT4Y773QtHsPenAJW7cji27QDfJuWxaq0A/zrpuF0b9OMd1ak8c+vttG/YwtG9kiwe3yMCQAbIFBLAUs25b79Fp58Et58E0pL4ZJL3DWd008PXEwNUPm/XRHhg+92MfPLLXy3M4uC4jKiw4IZdWprnvx5f0s6xtQTSza1FPBkU27nTnj2Wfj73+HAATdp5z33wLhxEGy/QCtTWFLK4s0ZfLhmD9szD/L69S5BP/fpJppHhHB+r7a0jT1572UypiGzZFNLDSbZlMvNhRkz3BxqW7dC165w110waZLrbjNVUlXGPfsl3+3MAqB/Yiw/7t2WC/q0tes9xtQhm67mZBcT4wYPbNwIb70F8fFuAEGnTvB//we7dwc6wgZNRJh72wg+njKSe0f3BBEen7+Bfy1PA6CopIzl2zIpK7M/vIzxF2vZVKLBtWwqUoXFi+GJJ9xaOKGh7h6dKVOgd+9AR3dS2J1VQJBA6+YRLPo+natnLCU+Jpzze7UmKT6G2KhQzjutDa2iw8gpKCa/uJQWkWGEhdjfZ8YcS8C60URkDPA0blnoF1X1TxW2hwOvAoOBDOByVd3qbXsAuA4oBW5X1fle+QzgQmCvqvbxqasVMBvoDGzFLQu9X9xcKU8DY4E8YJKqrqgq7gafbHxt3AjTpsHMmW5BtgsucO979Ah0ZCeNnIJiFqzfy4dr9/DZhnRyC0sAmHfHmZzWrjmvLdnKb99dA0B0WDAtosJoERXKi9ck0y42ki837WPJ5gxaRIXSMiqM/h1b0DUh+tA0PcY0FQFJNiISDHwPnA+kAcuAiaq61mefXwL9VPVmEZkAXKyql4tIL+BNYCjQHvgY6KGqpSIyEsgFXq2QbP4MZKrqn0TkfqClqv5aRMYCv8Ilm2HA06o6rKrYT6pkUy4jw80c/dRT7gbRmTPhZz8LdFQnHVUlt7CEA3nFtGkeQVhIEN/vyeHr1AwO5BVzIL+Y/XlFZOUV8+TPBxAbFcqzCzby5Eff49sL17FVJB/cfibNIkKPmB/OmMYsUMlmODBVVUd77x8AUNU/+uwz39tniYiEALuBBOB+33199/Pedwb+WyHZbABGqeouEWkHLFTVniLyvPf6zYr7HSv2kzLZlNuxAy67DL7+2g0ieOwx181m/KqsTMkpLCE9p5CvUjPYsDuH31/k/nne/uY3ZB4sYlTPBEb1TKBrQowlH9MoBWoGgQ7ADp/3abiWRaX7qGqJiGQBcV75VxWO7VDN+dr4JJDdQJsq4ugAHJFsRORG4EaATp06VXOqBqxjR1i0yA2RfuopWLoUZs+GDtV9feZEBAUJsZGhxEaG0q31kSPckhKiWbcrmz+8v44/vL+OxJaRXDnsFG4Z1TVA0RpT/xrl1U51zbVaNdlU9QVVTVbV5ISTcMXMI4SFwTPPuJtCV66EQYOa5HxrDcWd5/Xgoyln8cWvz+bRi/twWrvmh5bPLiwp5dqZS/nHolQ27snBXz0NxgSaP1s2O4GOPu8TvbLK9knzutFicQMFanJsRXtEpJ1PN9reWsTROE2YAP37u2s355/v5ly7//5GvYRBQ5bYMoorh53ClcNOOVS2O6uAnQfyefSDdTz6wTo6tIgksWUkd/+4J0O7tGLjnhzeXLqD6PBgosJCiA4PJjoshDN7xNO6WQQH8orYlVVATHgIUWHBRIeHEBFqN/yahsefyWYZ0F1EuuB+uU8Arqiwz1zgGmAJcCmwQFVVROYCb4jIk7gBAt2BpdWcr7yuP3nP7/qU3yYis3DdeFlVXa9pdE47zXWl3XgjPPigGzL92mtNapLPhuyUuGg+vOssdh7I57MN6Xy5aR/puYUEeZd00g7k81bKDg4WleDb6Jl14+m0bhbBZ9+nc8eslUfUGRcdxiuTh9KnQyxb9h1kW8ZBurWOoX1sJEFBdq3IBIa/hz6PBabhhj7PUNVHReQRIEVV54pIBPAaMBDIBCaoaqp37IPAZKAEuFNV53nlbwKjgHhgD/Cwqr4kInHAW0AnYBtu6HOmN/T5WWAMbujztapa5dX/k3qAwLGowvTpbtBAhw7w9ttuwTZzUlBVCorLOFhUQl5hKQnNwokMC+aHA/l8u+MAB4tKOVhYQm5hCdsz8rhvTE/iYsJ5dsFGnvjwewAiQ4NJSoima0IMf7i4D80jQsnKKyYiLMjmjzN1wqarqaVGmWzKff21G622Zw/89a9www1u8TbTKGXlFbN+dzab0w+yOT2XTXtz2ZGZx0dTziI4SPjNf75j1tLtdGwVRbeEGLq2jiEpPpoJQ90gmbIytdaQqTFbz8YcNmwYrFjhZhy46Sb44gs30WdUVKAjM34QGxXKsKQ4hiXFVbr9wr7tiI8JZ3N6Lpv35vL5pn0kxIQfSjbXv5rC6p1uXaHElpF0bBlFz7bN+Gn/9oAlI1Nzlmyaovh4eP99+MMf4He/g2++gX//22YdaILO6BbPGd3iD70vK1MO5Bcfen/uaa2Jiw4jbX8+y7ft57+rdjGoU4tDyeYnf/2CrLwiEltGkdgqksSWUQzs2IKzT219qD5LRgasG61SjbobraL58+HKK6GoyGYdMNUqKS0jp6CEltFhAExfuIlNe3JJ259P2v48dmUXMK5/e56eMBCAQb//iMjQYNcq8lpHpyfFcbrX0iotU4ItGTUa1o1mjm30aNeyuewyuPRSN4Dg4YchNjbQkZkGKCQ46FCiAfjlqG5HbC8qKSO/qBRwieTKYZ1I25/Pjsw8vti4jz05BRQUl3F6Uhy5hSUM+N2HtGsRQWILl4gSW0Zx7mmt6dMhlvScQmZ+uYXi0jKKS5WSsjJKSpVLBiUytEsrUtNz+eO89ZSUllFSphSXllFaptwyqivnnNqGopIyRCA02Ib6NwSWbMzRsw48+yycd55bIXT8eDjZb3I19SYsJOjQzNjBQcLdP+55xPbCklKKS11vSmmZctNZSYeS0Wffp7M3p5Do8GD6dIglK7+YFxalEhocREiwuOcgYXhX1yoqLlV2ZOZV2B5EXHQ4AAs37OVXb35Dv8RYBnVqycBOLRjYqSVtmtvieYFg3WiVaFLdaBUtXQr/+pe7hrNli7sBdORIl3guusglJmP8pLCkFEHqZCmHtT9k8+8VaazYvp81O7Mp8mZt+PSeUXSJj2b97mwOFpbSp0NzG/pdR2zocy016WRTThW+/Rbeecc91rgp9hk61CWeSy6B7t0DG6MxNVRYUsraH7L5dscBrh7emaAg4ddvr2J2yg7CgoPo1b45Azu1YHhSHD/u3RaAjXtyOFhUSrAIIq6lFhUWzClxbnXcPdkFFJeWERwk3j5CeGgQzSOa7sS3lmxqyZJNJTZsgP/8xyWeZctcWZ8+hxNPv352v445qezNKeCb7QdYsX0/32w/wKq0A7RtHsHCe88GYMILS/gqNfOIY3q3b877t58JwE//+sWhpcbLDe3SirduGg7A9a8sIyu/mLjocFrFhBEfHUbvDrGM9pLZ1n0HaR4ZSovI0EYzYs+STS1ZsqnG9u0wZ45LPJ9/DmVlkJTkks7FF8OQIbasgTnpFJeWsSMzj6QEN2v3N9v3sz+viLIyKFWlrEyJiQjhzO7uGubHa/eQebDIbfO2JzQLZ0yfdgD85j/fkZqeS+bBIjJyi8jMK2Jsn3Y8d+UgAPr/7kOy8osJDhJaRoURHxPGT/u359az3aCLTzfspVOrKDq1ijppBjlYsqklSza1sHcvzJ3rEs/HH7uF2yIiYOBA1+U2ZIh77tbNWj6mSSstUwqKS4kOd+Oy3vv2BzJyC8k4WMS+3CIycgs5o2sck0Z0IaegmL5TPwQgJEjoFBdFUnwMlw/pyPm92lBapmTlF9PKZ2RgQ2DJppYs2RynrCx3385XX7mutuXL3VLVAC1auMRT/hg6FNq3D2y8xjRQxaVlrN6ZRWr6QVL35bJ5r3uePKILE4Z2YuOeHM5/ahEto0JJSnBTDHVtHcPo3m3pEh9NTkExOw/kExYcdGiEYHhwMNHhwYT4sZVkyaaWLNnUkZISWLvWjXBbtsw9f/cdlLr7MGjf/nDrZ8gQSE622aiNqYG9OQXMXfkDqfsOsnlvLqn7DpKeU8g/rk7m/F5tWLB+D5NfPvp32OvXD2NEt3jeX7WLu/+18nAy8p7/9ovBnNau+XHHZcmmlizZ+FF+vlvQzTcBbdx4eHv37jBiBJx5JvzoR+69db8ZU63sgmLCgoOICA1mb3YBKdv2U1RSRlFJGYWl7nls37a0i41kzQ9ZvLvyB7etpIxib/u9o3vSsdXxz5NoyaaWLNnUs/37XZfb0qXu8eWXsG+f29amjUs6Z57pHv37Q7DdE2FMQ2TT1ZiGrWVLN2PBeee596qwfr0b6Vb++Pe/3bZmzeCMMw63fIYOhcjIwMVujKkRa9lUwlo2DdCOHW45hPLks3q1Kw8Lc9d6yls+I0a4wQjGmHpn3Wi1ZMnmJJCZ6brbPv/cJaGUFDfsGlzrJzbWJZ1jPVe1zVpKx+/77yEx0dZHaqIC1o0mImOAp3HLQr+oqn+qsD0ceBUYDGQAl6vqVm/bA8B1QClwu6rOr6pOETkHeAIIA5YD16lqiYiMAt4FtninfUdVH/HXZzb1pFUr+OlP3QMgL89d71myxN37k5UFBw645127YN26w2Xlo+GO5dRT4eyz3WPUKJuItDplZW59pCefhIUL3cwSc+ZA166Bjsw0IH5r2YhIMPA9cD6QBiwDJqrqWp99fgn0U9WbRWQCcLGqXi4ivYA3gaFAe+BjoHxlr6PqBNYD24BzVfV7EXkE2KaqL3nJ5h5VvbCmsVvLphFTdYnpwIHDycj3dXq6S1iffw4HD7pj+vQ5nHzOOsslOuNGFr76qpspfMMGN0nrL34Bzz/vvufZs+H88wMdpalHgWrZDAU2qWqqF8QsYDyw1mef8cBU7/XbwLMiIl75LFUtBLaIyCavPo5RZzpQpKrfe/t8BDwAvOSnz2ZOViIQHe0eHToce7/iYtc19+mn7vHii/DXv7rj+/c/nHxGjmx6a//s3QvPPQfTp7tRg4MHwxtvuPWQQkPh+uvdDOFjxsBjj8Hdd9vwdYM/J9zpAOzweZ/mlVW6j6qWAFlAXBXHHqt8HxAiIuUZ9VLAdy784SLyrYjME5HelQUrIjeKSIqIpKSnp9f8U5rGKTQUhg+H3/wGPvrIDc9etAimTnXXdaZPh3HjXCtnyBC47z6YNw9ycgIduf+sXesSSadO8Pvfu1GBCxe6+6UmTjw8H15SEixe7ObKu/de19rJywto6CbwTo7Z3aqhri9wAvCUiCwFcnDXegBWAKeoan/gr8CcY9Txgqomq2pygvXRm4rCw91ot4ceci2dAwdgwQJ48EE3F9y0aTB2rBvGnZwMv/qV+2t/yxbXpVQfcnPd/Urff++W+a4LqvDJJ+6z9e4Nr78O117rhqa/+67rVqys1RITA2+9BY8+Cm++6Yapb9tWNzGZk5I/u9F2cmTrItErq2yfNBEJAWJxAwWqOrbSclVdApwJICI/xrvGo6rZ5Tur6gciMl1E4lV13wl9OtO0RUQc7koD95f74sUuES1eDDNmuBVPwd2YOny4e5xxhut2OpERb6WlkJoKq1a56X9WrXKP1NTDiS0oyF1D6dq18kfzaqYkKSqCWbPcRf9vv4XWreGRR+CWWyA+vmZxiriWYf/+cMUVLgm//bZLUKbJ8ecAgRDcxfxzcQlhGXCFqq7x2edWoK/PAIFLVPXnXlfXGxweIPAJ0B2QY9UpIq1Vda83wu0D4FFVXSAibYE9qqoiMhR3begUreKD2wABc8JKSlwiWLLk8GPzZrctJMTNil2egIYPd11TlbUQMjKOTipr1hzulhJxU/r06wd9+7rWR26uO5fvY1+Fv63i413SSUo6Mgm1b+9Wav3rX+GHH6BXL5gyBa680iXY47Vhg1tifPNmN6Dg1lvtOk4jFLD7bERkLDANN0x5hqo+6o0US1HVuSISAbwGDAQygQk+F/8fBCYDJcCdqjrvWHV65Y8DF+K6Bv+mqtO88tuAW7x68oEpqrq4qrgt2Ri/2LvXzYhdnnyWLTucNNq1c0lnyBB3D1F5cvnhh8PHx8cfTir9+rlHr141u6clO/tw4klNPTIRbd/uhi/7Ovdcd2F/zJi6SwpZWXDVVfDeezB5srvuFR5eN3WbBsFu6qwlSzamXpSUuITi2/pJTXWzIvTqdXRiadPGP62BoiJ3PWXzZvc8bBgMGFD35wGX1KZOdQMMhg1z6yDZUhONhiWbWrJkYwJm/353cb2xr3T6zjtw9dVutod33nGtOnPSqyrZNIrRaMY0Gi1bNv5EA25Y9FdfuS7As85y9zGZRs2SjTEmMPr0cdetzj4bbrjBDRqoqyHbTUlpqZsf8L77Ds928corh2fAaCAs2RhjAqdVKzev2r33ugED553n3n/xhbuetW2b61osKQl0pA3LwYNu/rlrr4W2bd09YNOmuYEmaWkwaZIrv/56N2FtA7hcYtdsKmHXbIwJgDfegOuug4KCyrdHR7v7g2Jj3bPva9+yqCg3yCI83D1q+7qhdmPu2gX//S/MnQsff+y+pxYt3A2348fD6NHue1B1CWbGDHdj7cGD0KOHS0xXX+3XARk2QKCWLNkYEyB797pZF7Kz3SMr68jnqsrqaqqgNm3cKEDfR02HmNclVTdF0LvvugTz9deuvHNnl1zGjXMtmqqSY26uu29q5kw3uWxQkBvOfu21bsb0Oh56bsmmlizZGHMSKitzCSc/3137KSx0D9/XFd9XfJ2f75Ldd9+5m2fLW1ki0K2buybim4S6davbZcpLSlwX4ty57lF+I/CQIS65jB/vYjieIfAbN8LLL7vrOTt3Qlycu1n32mvrbKi7JZtasmRjjKG01P2y/+67w4/Vq2HTpsM3wUZEuFZPefLp08e1gHJyqn9kZx9dVn5RPyzM3Vg7fjxceGHVM5Qfz+f66CPX2pkzxyXZgQNd0rniCpeEjpMlm1qyZGOMOab8fNe95ZuEvvsOdu+u+rjISHdfUWWP5s0Pv+7b111/iYnx/2fJzHTXymbOhBUrXJK77Tb4y1+OqzpLNrVkycYYU2v79rmut6KioxNJTIybE68h+/Zbl3S6dnWzlh8HSza1ZMnGGGNqz2YQMMYYE1CWbIwxxvidJRtjjDF+Z8nGGGOM31myMcYY43eWbIwxxvidJRtjjDF+Z8nGGGOM39lNnZUQkXRgW6DjqEI8sC/QQVTB4jsxFt+JsfhOzInEd4qqJlS2wZLNSUhEUo51l25DYPGdGIvvxFh8J8Zf8Vk3mjHGGL+zZGOMMcbvLNmcnF4IdADVsPhOjMV3Yiy+E+OX+OyajTHGGL+zlo0xxhi/s2RjjDHG7yzZNEAi0lFEPhWRtSKyRkTuqGSfUSKSJSIrvcdD9RzjVhH5zjv3USvNifOMiGwSkVUiMqgeY+vp872sFJFsEbmzwj71/v2JyAwR2Ssiq33KWonIRyKy0XtueYxjr/H22Sgi19RjfI+LyHrvZ/gfEWlxjGOr/Pfgx/imishOn5/j2GMcO0ZENnj/Hu+vx/hm+8S2VURWHuPY+vj+Kv29Um//BlXVHg3sAbQDBnmvmwHfA70q7DMK+G8AY9wKxFexfSwwDxDgdODrAMUZDOzG3WwW0O8PGAkMAlb7lP0ZuN97fT/wWCXHtQJSveeW3uuW9RTfj4EQ7/VjlcVXk38PfoxvKnBPDf4NbAaSgDDg24r/n/wVX4XtfwEeCuD3V+nvlfr6N2gtmwZIVXep6grvdQ6wDugQ2KhqbTzwqjpfAS1EpF0A4jgX2KyqAZ8RQlUXAZkVbSS+GQAABPhJREFUiscDr3ivXwEuquTQ0cBHqpqpqvuBj4Ax9RGfqn6oqiXe26+AxLo+b039//buLdSqIo7j+PeXGoiGmIJdNE4Xn6RSESmxHiIkLYQKUhEqFULJLi9l4GtPPURoUmRXSjKiMh/MJIsIukmip6JAkyDleNRARQoz+/cwc2i13et4CtfsDf4+sNlrzYx7zR7nrP+ZWevMqmm/oZgJ7I2IfRHxB7CR1O7n1GD1kyTgHuDNc33coRrkvFKkDzrYdDlJPcA04Ks22TdK2i3pA0lTilYMAtgm6RtJD7TJvxz4pbK/n84EzIXU/4B3sv0GTIiIvrx9EJjQpky3tOVS0mi1nbP1hyatzNN8L9dMAXVD+90E9EfEnpr8ou3Xcl4p0gcdbLqYpNHAO8CjEXG8JXsnaWroemAtsKlw9WZHxHRgLvCgpJsLH/+sJF0IzAfebpPd6fY7Q6T5iq78WwRJq4E/gQ01RTrVH54DrgamAn2kqaputIjBRzXF2m+w80qTfdDBpktJGkHqEBsi4t3W/Ig4HhEn8vYWYISk8aXqFxEH8vsh4D3SVEXVAWBSZX9iTitpLrAzIvpbMzrdfhX9A9OL+f1QmzIdbUtJ9wN3AIvzyegMQ+gPjYiI/og4HRF/Aetrjtvp9hsO3AW8VVemVPvVnFeK9EEHmy6U53dfAn6IiKdrylySyyFpJun/8tdC9Rsl6aKBbdJF5O9aim0G7s13pd0AHKsM1Uup/W2yk+3XYjMwcGfPfcD7bcp8CMyRNDZPE83JaY2TdBvwODA/In6rKTOU/tBU/arXAe+sOe4OYLKkK/NodyGp3Uu5FfgxIva3yyzVfoOcV8r0wSbvfvDrf981Mps0lO0FduXXPGA5sDyXWQl8T7qz5ktgVsH6XZWPuzvXYXVOr9ZPwDrSXUDfAjMKt+EoUvAYU0nraPuRAl8fcIo0570MGAdsB/YAHwEX57IzgBcr/3YpsDe/lhSs317SXP1AP3w+l70M2DJYfyhUv9dz/+olnTQvba1f3p9Huvvqp5L1y+mvDvS7StlOtF/deaVIH/RyNWZm1jhPo5mZWeMcbMzMrHEONmZm1jgHGzMza5yDjZmZNc7BxqwgSaf17xWpz9kKxJJ6qisOm3WT4Z2ugNl55veImNrpSpiV5pGNWRfIzzN5Kj/T5GtJ1+T0Hkkf54Umt0u6IqdPUHq+zO78mpU/apik9fl5JdskjczlH87PMemVtLFDX9POYw42ZmWNbJlGW1DJOxYR1wLPAs/ktLXAaxFxHWkRzDU5fQ3waaSFRKeT/vIcYDKwLiKmAEeBu3P6E8C0/DnLm/pyZnW8goBZQZJORMToNuk/A7dExL68WOLBiBgn6QhpCZZTOb0vIsZLOgxMjIiTlc/oIT1zZHLeXwWMiIgnJW0FTpBWt94UeRFSs1I8sjHrHlGz/V+crGyf5p/rsreT1qqbDuzIKxGbFeNgY9Y9FlTev8jbn5NWKQZYDHyWt7cDKwAkDZM0pu5DJV0ATIqIT4BVwBjgjNGVWZP8241ZWSMl7arsb42Igdufx0rqJY1OFuW0h4BXJD0GHAaW5PRHgBckLSONYFaQVhxuZxjwRg5IAtZExNFz9o3MhsDXbMy6QL5mMyMijnS6LmZN8DSamZk1ziMbMzNrnEc2ZmbWOAcbMzNrnIONmZk1zsHGzMwa52BjZmaN+xumsb+UlJoVwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CELL. READ COMMENTS FIRST\n",
    "'''\n",
    "# this cell is not used to run the notebook unless you want to run a new fine tuning\n",
    "# compare the training and validation losses\n",
    "def plot_losses(train_loss, val_loss):\n",
    "\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_loss, '--', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(avgLosses_train, avgLosses_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331900db",
   "metadata": {
    "id": "331900db"
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72498ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1642618411778,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "e72498ed"
   },
   "outputs": [],
   "source": [
    "# define a function for peak signal-to-noise ratio (PSNR)\n",
    "def psnr(target, ref):    \n",
    "    # assume RGB image\n",
    "    target_data = target.astype(float)\n",
    "    ref_data = ref.astype(float)\n",
    "\n",
    "    diff = ref_data - target_data\n",
    "    diff = diff.flatten()\n",
    "\n",
    "    rmse = math.sqrt(np.mean(diff ** 2.))\n",
    "\n",
    "    return 20 * math.log10(255. / rmse)\n",
    "\n",
    "# define a function for mean squared error (MSE)\n",
    "def mse(target, ref):\n",
    "    err = np.sum((target.astype('float') - ref.astype('float')) ** 2)\n",
    "    # divide err by the shape of the image to get the error per pixel\n",
    "    err /= float(target.shape[0] * target.shape[1])\n",
    "    \n",
    "    return err\n",
    "\n",
    "# define a function that combines all three image quality metrics\n",
    "def compare_images(target, ref):\n",
    "    scores = []\n",
    "    scores.append(psnr(target, ref))\n",
    "    scores.append(mse(target, ref))\n",
    "    # multichannel is True: treat the last dimension of the array as channels\n",
    "    scores.append(ssim(target, ref, multichannel=True))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a842e",
   "metadata": {
    "id": "0d0a842e"
   },
   "source": [
    "## Reconstruct degraded image(s) outside the training/validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bae8f87",
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1642618416717,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "8bae8f87"
   },
   "outputs": [],
   "source": [
    "# define a function to make sure that the image is divisible by arbitrary scale\n",
    "def modcrop(img, scale):\n",
    "    tmpsz = img.shape\n",
    "    sz = tmpsz[0:2]\n",
    "    sz = sz - np.mod(sz, scale)\n",
    "    img = img[0:sz[0], 1:sz[1]]\n",
    "    return img\n",
    "\n",
    "# define a function to remove the first and last pixels after convolutions\n",
    "def shave(image, border):\n",
    "    img = image[border: -border, border: -border]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee88e7c6",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1642618420811,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "ee88e7c6"
   },
   "outputs": [],
   "source": [
    "# this function takes path of HR images for test, the interpolation method, and the scale used for degradation\n",
    "# the function produced the degraded images and returns the path of the folder containing them which is like:\n",
    "# test/LR Images\n",
    "# the high-res test images should be saved in a directory like: \"test/HR Images/\"\n",
    "\n",
    "def degradation(hr_path, interpolation_method, scale):\n",
    "    \n",
    "    # get the HR image names\n",
    "    names = os.listdir(hr_path)\n",
    "\n",
    "    for img in names:\n",
    "        \n",
    "        # read the high-res image and convert BGR to YCrCb and take the Y channel\n",
    "        # use modcrop function to be sure that the image is divisible by the scale\n",
    "        hr_img = cv2.imread('/content/drive/MyDrive/DIV2K/test/HR Images/' + img)\n",
    "        hr_img = modcrop(hr_img, scale)\n",
    "        hr_img_conv = cv2.cvtColor(hr_img, cv2.COLOR_BGR2YCrCb)\n",
    "        hr_img_Y = hr_img_conv[:, :, 0]\n",
    "        \n",
    "        h, w = hr_img_Y.shape\n",
    "        \n",
    "        # resize the high-res image to create a low-res one\n",
    "        lr_img = cv2.resize(hr_img_Y, (w//scale, h//scale), interpolation=interpolation_method)\n",
    "        lr_img = cv2.resize(lr_img, (w, h), interpolation=interpolation_method)\n",
    "        \n",
    "        # save the degraded image\n",
    "        hr_img_conv[:, :, 0] = lr_img\n",
    "        low_img = cv2.cvtColor(hr_img_conv, cv2.COLOR_YCrCb2BGR)\n",
    "        \n",
    "        if not os.path.isdir('/content/drive/MyDrive/DIV2K/test/LR Images'):\n",
    "            add = '/content/drive/MyDrive/DIV2K/test/LR Images'\n",
    "            os.makedirs(add)\n",
    "        else:\n",
    "            add = '/content/drive/MyDrive/DIV2K/test/LR Images'\n",
    "        cv2.imwrite(add +'/'+ img, low_img)\n",
    "        \n",
    "    return add\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c7e4a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1642618425299,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "00c7e4a2"
   },
   "outputs": [],
   "source": [
    "# set the required arguments for test function\n",
    "model = SRCNN(f1=9, f2=3, f3=5, n1=128, n2=64, c=1, mode='test')\n",
    "path_images_HR = '/content/drive/MyDrive/DIV2K/test/HR Images'\n",
    "path_params = '/content/drive/MyDrive/DIV2K/improved models/params_after_training_X2_BICUBIC.h5' # this should be the last weight saved on line 82 of the cell containing train_tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5952e531",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1642618428066,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "5952e531"
   },
   "outputs": [],
   "source": [
    "# test the trained/tuned model\n",
    "# the original images to be used for test should be stored in a directory like: \"test/HR Images/\"\n",
    "# so, please make the directory, \"test/HR Images/\"\n",
    "# and the corresponding low-res ones will be saved in \"test/LR Images/\" using the function degradation\n",
    "# the reconstructed images using the network will be saved in \"test/Reconstructed Images/\"\n",
    "# Note that you don't need to make \"test/LR Images/\" and \"test/Reconstructed Images/\" directories, \n",
    "# If they are not available, the program makes them\n",
    "\n",
    "def test(model, imageFolder_path, params_path, interpolation_method, scale):\n",
    "    \n",
    "    # restore the trained/tuned model parameters\n",
    "    model.load_state_dict(torch.load(params_path))\n",
    "    \n",
    "    # move the model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # put the model into the evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # get the degraded and reference images\n",
    "    hr_imageNames = os.listdir(imageFolder_path)\n",
    "    lr_imageNames = os.listdir(degradation(imageFolder_path,interpolation_method , scale))\n",
    "    \n",
    "    scores_deg_ref = []\n",
    "    scores_recon_ref = []\n",
    "    for i in range(len(hr_imageNames)):\n",
    "        \n",
    "        # read the high and low res images\n",
    "        degraded = cv2.imread('/content/drive/MyDrive/DIV2K/test/LR Images/' + lr_imageNames[i])\n",
    "        ref = cv2.imread('/content/drive/MyDrive/DIV2K/test/HR Images/' + hr_imageNames[i])\n",
    "    \n",
    "        # crop the reference image using modcrop function because the low-res image is cropped as well\n",
    "        # when suing degradation function\n",
    "        ref = modcrop(ref, scale)\n",
    "    \n",
    "        # convert the image to YCrCb - (srcnn trained on Y channel)\n",
    "        temp = cv2.cvtColor(degraded, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "        # create image slice and normalize  \n",
    "        Y = np.zeros((1, temp.shape[0], temp.shape[1], 1), dtype=float)\n",
    "        Y[0, :, :, 0] = temp[:, :, 0].astype(float) / 255\n",
    "    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # perform super-resolution with srcnn\n",
    "            Y_tensor=torch.tensor(Y,dtype=torch.float32,device=device)\n",
    "            Y_tensor = Y_tensor.resize(1,1,temp.shape[0],temp.shape[1])\n",
    "            pre = model(Y_tensor)\n",
    "    \n",
    "        # post-process output\n",
    "        pre *= 255\n",
    "        torch.clamp(pre, 0, 255)\n",
    "        pre = pre.type(torch.uint8)\n",
    "        pre_arr = pre.cpu().numpy()\n",
    "    \n",
    "        # copy Y channel back to image and convert to BGR\n",
    "        temp = shave(temp, 6)\n",
    "        temp[:, :, 0] = pre_arr[0, 0, :, :]\n",
    "        output = cv2.cvtColor(temp, cv2.COLOR_YCrCb2BGR)\n",
    "        \n",
    "        # save the reconstructed image to \"test/Reconstructed Images\"\n",
    "        if not os.path.isdir('/content/drive/MyDrive/DIV2K/test/Reconstructed Images'):\n",
    "            add = '/content/drive/MyDrive/DIV2K/test/Reconstructed Images'\n",
    "            os.makedirs(add)\n",
    "        else:\n",
    "            add = '/content/drive/MyDrive/DIV2K/test/Reconstructed Images'\n",
    "\n",
    "        cv2.imwrite(add + '/' + hr_imageNames[i], output)\n",
    "    \n",
    "        # remove border from reference and degraged image\n",
    "        ref = shave(ref.astype(np.uint8), 6)\n",
    "        degraded = shave(degraded.astype(np.uint8), 6)\n",
    "    \n",
    "        # image quality calculations\n",
    "        # scores of degraded and ref images is a list of lists: scores_deg_ref = [[psnr, mse, ssim(degraded, ref)], ...]\n",
    "        # scores of reconstructed and ref images is a list of lists: scores_deg_ref = [[psnr, mse, ssim(reconstruct, ref)], ...]\n",
    "        scores_deg_ref.append(compare_images(degraded, ref))\n",
    "        scores_recon_ref.append(compare_images(output, ref))\n",
    "    \n",
    "    # return images and scores\n",
    "    return scores_deg_ref, scores_recon_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f59585",
   "metadata": {
    "id": "89f59585"
   },
   "source": [
    "## Compare the original, degraded, and reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9d7885",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 1116,
     "status": "error",
     "timestamp": 1642618680312,
     "user": {
      "displayName": "Viktor Stavrinopoulos",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13023228101679582268"
     },
     "user_tz": -60
    },
    "id": "fe9d7885",
    "outputId": "485f036a-12be-4931-952b-83d2487300a6"
   },
   "outputs": [],
   "source": [
    "# for the high-res images in the \"test/HR Images\", test the super resolution model\n",
    "# the comparisons are saved in the directory \"test/results\", if not available, the program creates that path\n",
    "    \n",
    "# perform super-resolution\n",
    "scores_deg_ref, scores_recon_ref = test(model, path_images_HR, path_params, cv2.INTER_LINEAR, 2)\n",
    "\n",
    "# get the original, degraded, and reconstructed image paths\n",
    "ref = os.listdir('/content/drive/MyDrive/DIV2K/test/HR Images')\n",
    "deg = os.listdir('/content/drive/MyDrive/DIV2K/test/LR Images')\n",
    "rec = os.listdir('/content/drive/MyDrive/DIV2K/test/Reconstructed Images')\n",
    "\n",
    "# display images as subplots\n",
    "for i in range(len(ref)):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "    im_ref = cv2.imread('/content/drive/MyDrive/DIV2K/test/HR Images/' + ref[i])\n",
    "    axs[0].imshow(cv2.cvtColor(im_ref, cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title('Original')\n",
    "    im_deg = cv2.imread('/content/drive/MyDrive/DIV2K/test/LR Images/' + deg[i])\n",
    "    axs[1].imshow(cv2.cvtColor(im_deg, cv2.COLOR_BGR2RGB))\n",
    "    axs[1].set_title('Degraded')\n",
    "    axs[1].set(xlabel = 'PSNR: {}\\nMSE: {} \\nSSIM: {}'.format(scores_deg_ref[i][0],\n",
    "                                                       scores_deg_ref[i][1], scores_deg_ref[i][2]))\n",
    "    im_rec = cv2.imread('/content/drive/MyDrive/DIV2K/test/Reconstructed Images/' + rec[i])\n",
    "    axs[2].imshow(cv2.cvtColor(im_rec, cv2.COLOR_BGR2RGB))\n",
    "    axs[2].set_title('SRCNN')\n",
    "    axs[2].set(xlabel = 'PSNR: {} \\nMSE: {} \\nSSIM: {}'.format(scores_recon_ref[i][0],\n",
    "                                                        scores_recon_ref[i][1], scores_recon_ref[i][2]))\n",
    "    # remove the x and y ticks\n",
    "    for ax in axs:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # save the comparing images in \"test/results/\"\n",
    "    if not os.path.isdir('/content/drive/MyDrive/DIV2K/test/results'):\n",
    "        add = '/content/drive/MyDrive/DIV2K/test/results'\n",
    "        os.makedirs(add)\n",
    "    else:\n",
    "        add = '/content/drive/MyDrive/DIV2K/test/results'\n",
    "\n",
    "    fig.savefig('/content/drive/MyDrive/DIV2K/test/results/{}.png'.format(os.path.splitext(ref[i])[0])) \n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SRCNN_V3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "171px",
    "width": "258px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "251.28px",
    "left": "1012.8px",
    "top": "111.14px",
    "width": "184px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
